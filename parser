#! /usr/bin/env python3
from argparse import ArgumentParser
from pathlib import Path
from wordcloud import WordCloud
import re
import matplotlib.pyplot as plt
import json

def main():
    parser = ArgumentParser()
    parser.add_argument('--chat', required=True, type=str, help='File path to exported WhatsApp chat as .txt file.')
    # TODO: make the timestamp format less strict
    parser.add_argument("--start_date", required=True, type=str, help="Start date to being parsing stats (format: m/dd/yy).")
    # enable generating a graph
    parser.add_argument("-g", "--graph", action="store_true", help="Enabled outputting a graph.")
    parser.add_argument("--graph_bars", type=int, default=10)
    parser.add_argument("--graph_color", type=str, default="tab:blue")
    parser.add_argument("--graph_title", type=str, default="")
    parser.add_argument("--contacts", type=str, default="")
    parser.add_argument("--graph_file", type=str, default="graph.jpg", help="File to output the graph to.")
    # enable most common words per sender
    parser.add_argument("-w", "--words", action="store_true", help="Enable most common words by sender functionality")
    parser.add_argument("--words_count", type=int, default=5, help="The number of top senders you want the most common words for")
    args = parser.parse_args()
    # iterate through lines of the file until we find the target start date
    # once we've found the start date, then we begin counting statistics
    found_start = False
    unique_senders = set()
    msgs_per_sender = {}
    num_msgs = 0
    
    common_words = {}

    if args.contacts:
        contacts = json.load(open(args.contacts))

    chat_file = open(Path(args.chat))
    for line in chat_file:
        # lines that begin a new message should begin with a timestamp in this format
        # we only need the date portion to match the target date
        timestamp = re.search("^[\[]*([\d]*)[\/]([\d]*)[\/]([\d]*)", line)
        if not timestamp:
            continue
    
        # check if the line has our target date if we haven't found it yet
        timestamp_str = timestamp.group(0)

        # remove the opening bracket if it was caught by the regex
        timestamp_str = timestamp_str[1:] if timestamp_str[0] == "[" else timestamp_str
        if timestamp_str == args.start_date:
            found_start = True;
        
        if not found_start:
            continue
        
        # at this point, we'll begin parsing the messages for stats

        # to get the sender, remove the date from the beginning of the line
        # then, match any character up to the colon
        parseline = re.sub("^[\[]*([\d]*)[\/]([\d]*)[\/]([\d]*), [\d]*:[\d]*(:[\d]*){0,1} (AM|PM)[\]]*( [-])* ", "", line)
        sender_s = re.search("^([^:])+:", parseline)
        contents = re.sub("^([^:])+:", "", parseline)
        
        if contents == " <Media omitted>\n":
            contents = ''
        elif contents == " You deleted this message\n":
            contents = ''

        # messages such as people joining the chat don't have senders
        # we don't want to count them either, so increment num_msgs later
        if not sender_s:
            continue
        
        sender = sender_s.group(0)[:-1]

        if sender not in common_words.keys():
            common_words[sender] = (contents.strip() + " ")
        else:
            common_words[sender] += (contents.strip() + " ")
        
        if args.contacts and sender in contacts.keys():
            sender = contacts[sender]

        unique_senders.add(sender)
        
        # track messages per sender
        if sender not in msgs_per_sender.keys():
            msgs_per_sender[sender] = 1
        else:
            msgs_per_sender[sender] += 1
    
        num_msgs += 1
    
    # sort messages per sender
    sorted_msgs_by_sender = sorted(msgs_per_sender.items(), key=lambda x:x[1], reverse=True)

    # sorted_msgs_by_sender is a list of pairs
    # each pair is the ("name of the contact or the phone number", number of msgs sent)

    if args.graph:
        graph_msgs_per_person(sorted_msgs_by_sender, args.graph_bars, args.graph_title, args.graph_color, args.graph_file)
    
    if args.words:
        top_words(common_words, sorted_msgs_by_sender, args.words_count, args.start_date)
    
    print("RESULTS:\n")
    print("Unique senders: " + str(len(unique_senders)))
    print("Number of messages: " + str(num_msgs) + "\n")
    print("MESSAGES PER SENDER:\n")

    for item in sorted_msgs_by_sender:
        print(item[0] + ": " + str(item[1]))

def graph_msgs_per_person(msgs_per_sender, num_bars, title, color, output):
    # unpack the list of pairs into two tuples
    senders, num_msgs = zip(*msgs_per_sender[0:(num_bars-1)])
    
    # graph the sender names with the height of the bar being the number of msgs they have sent 
    plt.figure(figsize = (8, 5), facecolor='lightgray')
    plt.bar(senders, num_msgs, align="center", color=color)
    plt.title(title, fontsize="larger")

    # make the x labels rotated 45 degrees
    plt.xticks(rotation=45, ha='right')
    
    fig = plt.gcf()
    fig.tight_layout()

    plt.savefig(output, dpi=300)
    plt.close()

def top_words(words_by_sender, rankings, sender_count, start_date):
    # TODO find a way to eliminate words from the system that are insignificant

    # get the top ranked people that we will go through the data for 
    top_ranked = []
    if sender_count == 1:
        top_ranked = [list(rankings[0])[0]]
    else:
        for x, y in rankings[0:(sender_count)]:
            top_ranked.append(x)
    
    common_words = []
    try:
        common_words = json.load(open("common_words.json"))
    except:
        print("NOTE: no common_words.json file was found")
    
    # find word frequency per sender
    for x in top_ranked:
        # get list of every word the ranked sender has sent
        ranked_sender_all_words = (words_by_sender.get(x)).lower().split()
        ranked_sender_all_words_cleaned = [i for i in ranked_sender_all_words if i not in common_words]
        ranked_sender_all_words_cleaned_str = " ".join(ranked_sender_all_words_cleaned)

        # find the frequency of each word
        sender_words = {}
        #ranked_sender_all_words_cleaned = [word for word in ranked_sender_all_words_cleaned if len(word) > 1]
        for word in ranked_sender_all_words_cleaned:
            if word not in sender_words.keys():
                sender_words[word] = 1
            else:
                sender_words[word] += 1

        wordcloud = WordCloud(width=800, height=660).generate(ranked_sender_all_words_cleaned_str)
        
        plt.figure(figsize=(10,8))
        plt.axis("off")
        plt.title(x + " Most Common Words since " + start_date, fontsize=13)
        plt.imshow(wordcloud)
        plt.savefig("".join(x.split()) + "_Most Common_Words_Since_"+"-".join(start_date.split("/"))+".png", bbox_inches='tight')
        plt.close()

if __name__ == '__main__':
    main()
